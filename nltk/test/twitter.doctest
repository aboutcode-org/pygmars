.. Copyright (C) 2001-2014 NLTK Project
.. For license information, see LICENSE.TXT

.. NB Doctests that depend on the Twitter streaming API are expected
.. to fail. To adapt Heraclitus: No Twitter client steps into the same
.. stream twice.

====================
 Processing Twitter
====================

Introduction
============

There is widespread interest in processing and analysing Twitter data,
not least by means of NLP, and although there are a variety of client
libraries for accessing the Twitter APIs, it can be time-consuming to
bring together the various modules for processing the data that they
allow you to access. The NLTK Twitter package is intended to make it
easier for you to get to the interesting part of analysing Tweets.


Twitter provides two main APIs, namely the REST API
and the streaming API. If we are primarily interested in collecting
Tweets for analysis (rather than, say, publishing our own Tweets), then the
two approaches both have limitations. The REST API allows us to search
past Tweets, but only within the strict bounds allowed by `Twitter's rate-limit policy
<https://dev.twitter.com/docs/rate-limiting/1.1>`_.\ [#]_ 
The streaming API will allow us to collect an
unlimited volume of Tweets, but with two caveats: 

* We can only collect Tweets as they are published, not search
  retrospectively; and

* we can only access at most 1% of the public Twitter stream (unless
  we somehow have obtained a higher level of access, which is hard to come by).



.. [#] At the time of writing, the REST API limits searches to 180 queries per 15 minute window.


The NLTK Twitter Client
=======================

NLTK supports access to Twitter via the module
`nltk.twitter.twitterclient`. This is in turn depends on a third party
Python Twitter client, namely `Twython
<https://pypi.python.org/pypi/twython>`_. Although there are a number
of Python packages for calling the Twitter API, Twython has
appropriate functionality, is actively maintained, has Python 3
support, and has `good documentation
<https://twython.readthedocs.org>`_. 

Twython provides a Pythonic way of accessing the **endpoints** that
Twitter uses to expose its API. A full endpoint might be a URL such as
``https://stream.twitter.com/1.1/statuses/filter.json?delimited=length&track=twitterapi``;
however, we standardly abbreviate endpoints like this as 'statuses/filter'.

In order to gain access to the Twitter API, it is necessary to first
`register your application with Twitter <https://dev.twitter.com/apps>`_. There are two main options. OAuth 1
is for user-authenticated API calls, and allows sending status
updates, direct messages, etc, whereas OAuth 2 is for application-authenticated calls,
where read-only access is sufficient. Although OAuth 2 sounds more
appropriate for the kind of tasks envisaged within NLTK, it turns out
that access to Twitter's streaming API requires OAuth 1, so the
following discussion assumes that after logging into Twitter via a
user account, you have obtained **Read and Write** access for your
application (as specified on the *Permissions* tab of Twitter's
Application Management screen). This will give you four distinct keys,
which you should store in a text file with the following structure::

  API key=YOUR API KEY
  API secret=YOUR API SECRET
  Access token=YOUR ACCESS TOKEN
  Access token secret=YOUR ACCESS TOKEN SECRET

By default, ``twitterclient`` will search for this data in a file `credentials.txt`
located in the directory specified by the environment variable
TWITTER. However, both of these choices can be overridden, namely
by setting the parameters to `authenticate()`, which is introduced
below.

When Tweets are received, via either the streaming or the REST API,
they can be passed to the terminal for casual inspection and
debugging, or stored in one or more files. (In principle, they could
also be stored in a database, though support for this has not been
implemented yet.) The methods for dealing with Tweets are provided by
extensions of the `TweetHandler` class, such as `TweetViewer`: this
displays the text of Tweets on the terminal. (While it would be
possible of course to display the whole Tweet object, this contains
more information than one typically needs for most purposes; see the
`Twitter documentation on Tweets
<https://dev.twitter.com/docs/platform-objects/tweets>`_ for an
exhaustive description.)  Initialisation can also set a limit to the
number of Tweets received (e.g., by ``limit=10``). The following
example illustrates loading the API keys from a file, and passing them
to the streaming client class `Streamer` which is initialised with an
instance of `TweetViewer`. The latter only shows the ``text`` field of
Tweets, since that is usually the main focus of interest when we are
doing a visual inspection of the Twitter stream.

>>> from nltk.twitter.twitterclient import authenticate, Streamer, TweetViewer, TweetWriter
>>> import os
>>> os.environ['TWITTER']
'/Users/ewan/twitter'
>>> oauth = authenticate('credentials.txt')
>>> handler = TweetViewer(limit=10)
>>> client = Streamer(handler, **oauth)

One option offered by the streaming API is to take a 1% sample of the public
stream; we can send a request to the API endpoint ``statuses.sample()`` by delegating to Twython's ``statuses.sample()`` method:

>>> client.statuses.sample()
Be alright #MTVHottest Justin Bieber
RT @PriceRunner: RT to show your support for the Brits at the #CommonwealthGames @weRengland @TeamWales @Team_Scotland @Glasgow2014
iki y√ºzl√º dostlar :)
@colgadosfutbol Ni en Vallecas hizo nada
I do have a period of time when I do dream of the girls like every night but then I have another period of time without, &amp; I'm in that rn.
Wag mo na kase ipagsiksikan sarili mo boy.
I wish I can Vote TT _ TT #Ïù∏ÌîºÎãàÌä∏
RT @Engabdallahelha: ÿßŸÜÿß ŸÇŸÑŸäŸÑ ÿßŸÑŸÉŸÑÿßŸÖ... ŸàÿØŸá ÿ®ŸäÿÆŸÑŸä ÿßŸÑŸÜÿßÿ≥ ÿ™ÿÆÿßŸÅ ŸÖŸÜŸä
Hi @JaiBrooks1 !üê∑How are you sunshine?‚òÄPls, follow @rockingchadwick !üòÑShe loves you so much!üòçüòò‚ô•(DON'T IGNORE THAT TWEET) üò°üò°x32
RT @ka3am_al3ayel: @Bander65861835 http://t.co/qP9vuhIac5

Although this approach seems on the face of it to give an interesting
view on the Twitterverse, little information is available about the
sampling method Twitter uses, or how representative it is; see
[Morstatter2013]_ for a discussion of these issues.

The `'statuses/filter' endpoint
<https://dev.twitter.com/docs/api/1/post/statuses/filter>`_ allows us
to filter the stream to track keywords, user IDs and locations. The
following example illustrates the use of keywords:

>>> keywords = "Pistorius, #OscarTrial, gerrie nel"
>>> client.statuses.filter(track=keywords)   # doctest: +SKIP
RT @DonadieuP: Roux √† son tour critique les t√©moins de l'accusation et notamment les policiers dont les t√©moignages ont parfois √©t√© douteux‚Ä¶
RT @OscarTrial199: #oscartrial Nel: defence would say 90% of the messages were 'boo boo Baba xxx kiss kiss'...that's true, but it's the 10%‚Ä¶
Creo que soy la √∫nica persona en el mundo que siente peca√≠to por Oscar #Pistorius
RT @DonadieuP: Roux :" l'accusation a √©galement laiss√© de c√¥t√© des t√©moins-cl√©s" comme H.Botha le policier qui avait d√©truit la sc√®ne de cr‚Ä¶
RT @barrybateman: #OscarTrial Nel: his intention when he fired those shots was to kill a human being. BB
Pistorius looking like he's starring in the hottest political thriller in the box-office http://t.co/pY2sYJuG1I
Prosecutor Calls Pistorius 'Deceitful Witness' http://t.co/62MQ13qvoV #Case #CourtJudge #Murder #Shot
Both lawyers in the #OscarTrial are brilliant. ..These closing arguments have got me confused.
#urgent Afrique du Sud : le parquet requiert une condamnation pour meurtre contre Oscar Pistorius http://t.co/tTx2Lr2Rmx"
"@RFI: #urgent Afrique du Sud : le parquet requiert une condamnation pour meurtre contre Oscar Pistorius http://t.co/tTx2Lr2Rmx"

In order to filter the stream for users, we employ the ``follow``
parameter key, followed by one or more user IDs. Somewhat confusingly,
user IDs are *not* the same as the 'screen names' that appear on
Tweets. For example, the user ID corresponding to ``@BBCNews`` is
612473. There are various websites that will map screen names into
user IDs, but we can do this ourselves with a few lines of code using
the Twitter REST API, via NLTK's ``twitterclient.Query`` class.

>>> from nltk.twitter.twitterclient import Query
>>> def get_userid(screen_name):
...    oauth = authenticate()
...    client = Query(**oauth)
...    info = client.show_user(screen_name=screen_name)
...    return info['id_str']
>>> get_userid('BBCNews')
'612473'

Using this technique to obtain user IDs for ``@BBCNews``,
``@GuardianNews`` and ``@ReutersUS`` we can monitor Tweets that are
from, or mention, these users:

>>> client.statuses.filter(follow='612473,788524,15108530')
RT @BBCScotlandNews: Much excitement in US town of Boring as it celebrates pairing with Perthshire village of Dull http://t.co/zVh6Lfx24i h‚Ä¶
RT @BBCNews: "Left out in the cold to die"? High altitude suffering for Pakistan's little-known K2 mountain pioneer http://t.co/7UB3UNqkvB
RT @BBCNews: Infested luggage &amp; Holy Grail relics: Test your knowledge in this week's @BBCNewsMagazine quiz http://t.co/badiwdRNxu http://t‚Ä¶
RT @ReutersUS: Americans worry that illegal migrants threaten their way of life and the economy, poll shots: http://t.co/43DY6Gcjg6 http://‚Ä¶
@BBCNews how awesome !
RT @guardiannews: UK trade deficit widens as exports fall again http://t.co/mTnkeFT9eq
RT @BBCNewsUS: Alliance Boots says it expects there to be no UK job losses after US firm Walgreens takes over http://t.co/bxIDdQMT34
RT @BBCNews: Running track on 23rd floor? Firms' futuristic office plans http://t.co/AvvglAmPuP &amp; http://t.co/CLTFAlPTTl
RT @BBCWalesNews: ‚ñ∫ 'Fantastic' http://t.co/YABbt70PFb
@ReutersUS Ask a native American Indian about illegal immigration. We are ALL immigrants !

By calling a different handler, namely `TweetWriter`, we can get
full Tweets written to files of a given length, again using the ``limit``
parameter. By default, the files will be written to the subdirectory
specified by TWITTER, but again this can be overriden. If the
parameter ``repeat`` is set to ``False`` (default), Tweets will stop
being collected once the limit is reached. Otherwise, `TweetWriter`
will continue creating new files of the specified limit until the
process is terminated externally.

>>> handler = TweetWriter(limit=20, repeat=True)
>>> client = Streamer(handler, **oauth)
>>> client.statuses.sample() # doctest: +SKIP
Writing to /Users/ewan/twitter/tweets.20140805-135210.json
Written 20 tweets
<BLANKLINE>
Writing to new file /Users/ewan/twitter/tweets.20140805-135212.json
Written 20 tweets
<BLANKLINE>
Writing to new file /Users/ewan/twitter/tweets.20140805-135213.json
Written 20 tweets
<BLANKLINE>
Writing to new file /Users/ewan/twitter/tweets.20140805-135214.json
Written 20 tweets
...

Corpus Reader
=============

Given a collection of Tweets in one or more files,
`TwitterCorpusReader` offers some standard methods for reading
them. The method ``full_tweets()`` returns a list of the full Tweets as
dictionaries derived by deserialising the JSON strings in the corpus
file(s). We assume that each JSON string is on a line of its own in
the file. 

>>> from nltk.corpus import TwitterCorpusReader
>>> root = os.environ['TWITTER']
>>> reader = TwitterCorpusReader(root, '.*\.json')
>>> for t in reader.full_tweets()[:1]:
...   print(t)
{'place': None, 'entities': {'symbols': [], 'hashtags': [],
'user_mentions': [], 'urls': [{'expanded_url':
'http://www.visir.is/rolf-toft--var-godur-i-maganum-i-gaer/article/2014140809962',
'indices': [37, 59], 'url': 'http://t.co/jIvWHkp2wk', 'display_url':
...

More immediately useful is the ``tweets()`` method, which just returns
the text of each Tweet:

>>> for t in reader.tweets()[:15]:  # doctest: +SKIP
...   print(t)
Skora√∞i aftur en f√©kk ekki √≠ magann. http://t.co/jIvWHkp2wk
Ouch, likewise üëé
@my_future_cat —ç—ç—ç—ç..–Ω—É—É—É—É...–º–Ω–µ 17, –≥—ã
@__foreignmarley can you bring the speakers please
RT @goodhealth: Wake up to a healthy breakfast! Make your own granola, on-the-go bars, and parfaits with these #recipes --&gt; http://t.co/Q6R‚Ä¶
#Almer√≠a #Trabajo Se necesita repartidores de publicidad: Almer√≠a Incorporamos 35 personas para distribuir pro... http://t.co/uvbPLNRNad
@Nenenesssaaa ikaw yon, hindi ako. :p
84x  #MTVHottest Justin Bieber #BestFandom2014 Beliebers
Êó•Êú¨Âè≤Â´å„ÅÑ„Å†„Çè‚òÜÂΩ°.„ÄÇ
with hulya..
My life is so boring and sadüòí
RT @KUDUNEWS: Karrueche Posts Cryptic Gun Range Pics on IG (@Karrueche @Chrisbrown) - #celebrities #gossip #kudunews http://t.co/U4Ej91H63p
RT @Malefica_1: El lobo siempre ser√° el malo si Caperucita es quien cuenta la historia. #Malefica
RT @miilkkk: üòíüíØ http://t.co/GxXkjHV9KG
Phumolo asks "How much for these apprenticeships? "...Is he  joking? http://t.co/nud7fYA3dP shows SCAM FREE things, you don't pay for jobs!

The default tokeniser for Tweets is `TweetTokenizer`, which is called
by the ``tokenised_tweets()`` method.

>>> for t in reader.tokenised_tweets()[:15]:
...   print(t)
['Skora', '√∞i', 'aftur', 'en', 'f√©kk', 'ekki', '√≠', 'magann', '.', 'http://t.co/jIvWHkp2wk']
['Ouch', ',', 'likewise', 'üëé']
['@my_future_cat', '—ç—ç—ç—ç', '..', '–Ω—É—É—É—É', '...', '–º–Ω–µ', '17', ',', '–≥—ã']
['@__foreignmarley', 'can', 'you', 'bring', 'the', 'speakers', 'please']
['RT', '@goodhealth', ':', 'Wake', 'up', 'to', 'a', 'healthy', 'breakfast', '!', 'Make', 'your', 'own', 'granola', ',', 'on-the-go', 'bars', ',', 'and', 'parfaits', 'with', 'these', '#recipes', '-->', 'http://t.co/Q6R‚Ä¶']
['#Almer√≠a', '#Trabajo', 'Se', 'necesita', 'repartidores', 'de', 'publicidad', ':', 'Almer', '√≠a', 'Incorporamos', '35', 'personas', 'para', 'distribuir', 'pro', '...', 'http://t.co/uvbPLNRNad']
['@Nenenesssaaa', 'ikaw', 'yon', ',', 'hindi', 'ako', '.', ':p']
['84x', '#MTVHottest', 'Justin', 'Bieber', '#BestFandom2014', 'Beliebers']
['Êó•Êú¨Âè≤Â´å„ÅÑ„Å†„Çè', '‚òÜ', 'ÂΩ°', '.', '„ÄÇ']
['with', 'hulya', '..']
['My', 'life', 'is', 'so', 'boring', 'and', 'sad', 'üòí']
['RT', '@KUDUNEWS', ':', 'Karrueche', 'Posts', 'Cryptic', 'Gun', 'Range', 'Pics', 'on', 'IG', '(', '@Karrueche', '@Chrisbrown', ')', '-', '#celebrities', '#gossip', '#kudunews', 'http://t.co/U4Ej91H63p']
['RT', '@Malefica_1', ':', 'El', 'lobo', 'siempre', 'ser', '√°', 'el', 'malo', 'si', 'Caperucita', 'es', 'quien', 'cuenta', 'la', 'historia', '.', '#Malefica']
['RT', '@miilkkk', ':', 'üòí', 'üíØ', 'http://t.co/GxXkjHV9KG']
['Phumolo', 'asks', '"', 'How', 'much', 'for', 'these', 'apprenticeships', '?', '"', '...', 'Is', 'he', 'joking', '?', 'http://t.co/nud7fYA3dP', 'shows', 'SCAM', 'FREE', 'things', ',', 'you', "don't", 'pay', 'for', 'jobs', '!']

Since this tokeniser assumes that words are delimited by
whitespace, it fails to deal with writing systems such as Japanese
that use different conventions for delimiting words.


Twitter Corpora
===============

Restrictions imposed by Twitter make it awkward to use
standard approaches for storing and distributing Tweet
corpora. More specifically, according to Twitter's `Terms of Service <https://dev.twitter.com/terms/api-terms>`_:

	 If you provide downloadable datasets of Twitter Content or an
	 API that returns Twitter Content, you may only return IDs
	 (including Tweet IDs and user IDs).

This is motivated, at least in part, by the need to honour
the right of users to delete their tweets; deletion would not be truly
possible if the tweets in question survived in corpora redistributed
by third parties. Consequently, it is not possible for NLTK to include a corpus of
Tweets within its collection of downloadable resources. However, we
can adopt the method suggested by Twitter above
of distributing a corpus of Tweet IDs, and leave
it to individuals to retrieve the full Tweets corresponding to those
IDs. Although this is a workable alternative, it should be be borne in
mind that there are two potential problems:

* The reconstructed corpus is not guaranteed, or even likely, to be
  identical to the original Tweet corpus, due to Tweets having been deleted
  in the intervening time.

* If the corpus is large, then Twitter's rate-limiting policy might
  make the process of reconstructing the corpus excessively
  time-consuming. 

See [McReadie21012]_ for more discussion of these issues.

The `Twitter API
<https://dev.twitter.com/docs/api/1.1/get/statuses/lookup>`_ refers to
the process of "returning fully-hydrated tweet objects" from a list of
Tweet IDs, so we've named the process of converting full Tweets into
IDs as *dehydration*. The function ``dehydrate()`` returns an iterator
over such IDs.

>>> from nltk.twitter.twitterclient import dehydrate
>>> TWEETS = os.path.join(os.environ['TWITTER'], 'tweets.20140801-150110.json')
>>> for id_str in list(dehydrate(TWEETS))[:10]:
...     print(id_str)
    495207619507019776
    495207619519582209
    495207619523805184
    495207619527966720
    495207619498618881
    495207619494424576
    495207619523788800
    495207619506601984
    495207619527995392
    495207619502833664

We can of course also save this list to a file.

>>> IDFILE = os.path.join(os.environ['TWITTER'], 'tweet_ids.txt')
>>> with open(IDFILE, 'w') as f:
...     for id_str in dehydrate(TWEETS):
...         print(id_str, file=f)


In order carry out hydration, i.e., converting Tweet IDs into full
Tweet objects, we need to call the Twitter REST API, which we can do
using the NLTK client `Query`. The authentication steps for this are
the same as we saw above for the streaming API. In order to write the
result of hydrating Tweets from the list of Tweet IDs, we need to
invoke ``json.dumps()``, which serialises the Tweets into JSON
strings.

>>> from nltk.twitter.twitterclient import Query
>>> oauth = authenticate()
>>> client = Query(**oauth)
>>> FULL = os.path.join(os.environ['TWITTER'], 'rehydrated.json')
>>> client.hydrate(IDFILE, FULL)
Counted 20 Tweet IDs in /Users/ewan/twitter/tweet_ids.txt.
Written 19 Tweets to file /Users/ewan/twitter/rehydrated.json of length 69053 bytes

References
==========

.. [McReadie21012] Richard
   McCreadie, Ian Soboroff, Jimmy Lin, Craig Macdonald, Iadh Ounis and
   Dean McCullough (2012) *On Building a Reusable Twitter Corpus*
   `PDF <http://www.dcs.gla.ac.uk/~craigm/publications/mccreadie12tweets.pdf>`__

.. [Morstatter2013] Fred Morstatter, Juergen Pfeffer, Huan Liu and Kathleen
   M. Carley (2013)
   *Is the Sample Good Enough? Comparing Data from Twitter‚Äôs
   Streaming API with Twitter‚Äôs Firehose*
   `PDF <http://www.public.asu.edu/~fmorstat/paperpdfs/icwsm2013.pdf>`__

