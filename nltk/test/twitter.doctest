.. Copyright (C) 2001-2014 NLTK Project
.. For license information, see LICENSE.TXT

====================
 Processing Twitter
====================


Twitter Corpora
===============

Although there is widespread interest in processing and analysing
Twitter data, restrictions imposed by Twitter make it awkward to use
standard NLP approaches for storing and distributing reusable tweet
corpora. More specifically, according to Twitter's `Terms of Service <https://dev.twitter.com/terms/api-terms>`_:

	 If you provide downloadable datasets of Twitter Content or an
	 API that returns Twitter Content, you may only return IDs
	 (including Tweet IDs and user IDs).

This is motivated, at least in part, by the need to honour
the right of users to delete their tweets; deletion would not be truly
possible if the tweets in question survived in corpora redistributed
by third parties.

Consequently, it is not possible for NLTK to include a corpus of
Tweets within its collection of downloadable resources. However, we
can adopt the standard
workaround [McReadie21012]_ of distributing a corpus of Tweet IDs, and leave
it to individuals to retrive the full Tweets corresponding to those
IDs. Although this is a workable alternative, it should be be borne in
mind that there are two potential problems:

* The reconstructed corpus is not guaranteed, or even likely, to be
  identical to the original Tweet corpus, due to Tweets having been deleted
  in the intervening time.

* If the corpus is large, then Twitter's rate limiting policy might
  make the process of reconstructing the corpus excessively
  time-consuming. 

Twitter provides two APIs, namely the REST API
and the Streaming API. If we are primarily interested in collecting
Tweets for analysis (rather, say, than publishing Tweets), then the
two approaches both have limitations. The REST API allows us to search
past Tweets, but only in a severely limited manner
[***Amplify***]. The Streaming API will allow us to collect an
unlimited volume of Tweets, but with two caveats: 

* We can only collect Tweets as they are published, not search
  retrospectively; and

* we can only access at most 1% of the public Twitter stream (unless
  we somehow have priviliged access to the Twitter 'firehose').

One option of the Streaming API is to take a 1% sample of the public
stream (using the API endpoint ``statuses.sample()``). Little
information is available however about what the sampling methods is,
or how representative it is; see [Morstatter2013]_ for a discussion
of these issues.


The Twitter Client
==================

NLTK supports access to Twitter via the module
`nltk.misc.twitterclient`. This is in turn depends on a third party
Python Twitter client, namely `Twython
<https://pypi.python.org/pypi/twython>`_. Although there are a number
of Python packages for calling the Twitter API, Twython has
appropriate functionality, is actively maintained, has Python 3
support, and has `good documentation
<https://twython.readthedocs.org>`_.

In order to gain access to the Twitter API, it is necessary to first
register your application with
`<https://dev.twitter.com/apps>`_. There are two main options. OAuth 1
is for user authenticated API calls, and allows sending status
updates, etc, whereas OAuth 2 is for application authenticated calls,
where read-only access is sufficient. Although OAuth 2 sounds more
appropriate for the kind of tasks envisaged within NLTK, it turns out
that access to Twitter's streaming API requires OAuth 1, so the
following discussion assumes that after logging into Twitter via a
user account, you have obtained **Read and Write** access for your
application (as specified on the *Permissions* tab of Twitter's
Application Management screen). This will give you four distinct keys,
which you should store in a text file with the following structure::

  API key=YOUR API KEY
  API secret=YOUR API SECRET
  Access token=YOUR ACCESS TOKEN
  Access token secret=YOUR ACCESS TOKEN SECRET

By default, this data will looked for in a file `credentials.txt`
located in the directory specified by the environment variable
TWITTERHOME. However, both of these choices can be overridden, namely
by setting the parameters to `authenticate()`, which is introduced
below.

When Tweets are received, via either the streaming or the REST API,
they can be passed to the terminal for casual inspection or stored in
one or more files. (In principle, they could also be stored in a
database, though this has not been implemented yet.) The methods for
dealing with Tweets are provided by extensions of the `TweetHandler`
class, such as `TweetViewer`: this displays the Tweets on the
terminal. Initialisation can also set a limit to the number of Tweets
received (e.g., ``limit=10``). The following example illustrates
taking the API keys from a file, and passing them to the streaming
client class `Streamer` together with an instance of `TweetViewer`.

>>> from nltk.misc.twitterclient import authenticate, Streamer,
    TweetHandler, TweetWriter
>>> import os
>>> os.environ[TWITTERHOME]
/Users/ewan/twitter
>>> oauth = authenticate('credentials.txt')
>>> handler = TweetViewer(limit=10)
>>> client = Streamer(handler, **oauth)
>>> client.statuses.sample()  # doctest: +SKIP
Be alright #MTVHottest Justin Bieber
RT @PriceRunner: RT to show your support for the Brits at the #CommonwealthGames @weRengland @TeamWales @Team_Scotland @Glasgow2014
iki y√ºzl√º dostlar :)
@colgadosfutbol Ni en Vallecas hizo nada
I do have a period of time when I do dream of the girls like every night but then I have another period of time without, &amp; I'm in that rn.
Wag mo na kase ipagsiksikan sarili mo boy.
I wish I can Vote TT _ TT #Ïù∏ÌîºÎãàÌä∏
RT @Engabdallahelha: ÿßŸÜÿß ŸÇŸÑŸäŸÑ ÿßŸÑŸÉŸÑÿßŸÖ... ŸàÿØŸá ÿ®ŸäÿÆŸÑŸä ÿßŸÑŸÜÿßÿ≥ ÿ™ÿÆÿßŸÅ ŸÖŸÜŸä
Hi @JaiBrooks1 !üê∑How are you sunshine?‚òÄPls, follow @rockingchadwick !üòÑShe loves you so much!üòçüòò‚ô•(DON'T IGNORE THAT TWEET) üò°üò°x32
RT @ka3am_al3ayel: @Bander65861835 http://t.co/qP9vuhIac5

By setting a different handler, namely `TweetWriter`, we can get
Tweets written to files of a given length, again using the ``limit``
parameter. By default, the files will be written to the subdirectory
specified by TWITTERHOME, but again this can be overriden. If the
parameter ``repeat`` is set to ``False`` (default), Tweets will stop
being collected once the limit is reached. Otherwise, `TweetWriter`
will continue creating new files of the specified limit until the
process is terminated externally.

>>> handler = TweetWriter(limit=20, repeat=True)
>>> client = Streamer(handler, **oauth)
>>> client.statuses.sample() # doctest: +SKIP
Writing to /Users/ewan/twitter/tweets.20140805-135210.json
Written 20 tweets
<BLANKLINE>
Writing to new file /Users/ewan/twitter/tweets.20140805-135212.json
Written 20 tweets
<BLANKLINE>
Writing to new file /Users/ewan/twitter/tweets.20140805-135213.json
Written 20 tweets
<BLANKLINE>
Writing to new file /Users/ewan/twitter/tweets.20140805-135214.json
Written 20 tweets
...

Corpus Reader
=============

Given a collection of Tweets in one or more files,
`TwitterCorpusReader` offers some standard methods for reading
them. The method ``jsonlist()`` returns a list of the full Tweets as
dictionaries derived by deserialising the JSON strings in the corpus
file(s). We assume that each JSON string is on a line of its own in
the file. 

>>> from nltk.corpus import TwitterCorpusReader
>>> root = os.environ['TWITTERHOME']
>>> reader = TwitterCorpusReader(root, '.*\.json')
>>> for t in reader.jsonlist[:15]:  # doctest: +SKIP
...   print(t)
{'place': None, 'entities': {'symbols': [], 'hashtags': [],
'user_mentions': [], 'urls': [{'expanded_url':
'http://www.visir.is/rolf-toft--var-godur-i-maganum-i-gaer/article/2014140809962',
'indices': [37, 59], 'url': 'http://t.co/jIvWHkp2wk', 'display_url':
...

More immediately useful is the ``tweets()`` method, which just returns
the text of each Tweet:

>>> for t in reader.tweets[:15]:  # doctest: +SKIP
...   print(t)
Skora√∞i aftur en f√©kk ekki √≠ magann. http://t.co/jIvWHkp2wk
Ouch, likewise üëé
@my_future_cat —ç—ç—ç—ç..–Ω—É—É—É—É...–º–Ω–µ 17, –≥—ã
@__foreignmarley can you bring the speakers please
RT @goodhealth: Wake up to a healthy breakfast! Make your own granola, on-the-go bars, and parfaits with these #recipes --&gt; http://t.co/Q6R‚Ä¶
#Almer√≠a #Trabajo Se necesita repartidores de publicidad: Almer√≠a Incorporamos 35 personas para distribuir pro... http://t.co/uvbPLNRNad
@Nenenesssaaa ikaw yon, hindi ako. :p
84x  #MTVHottest Justin Bieber #BestFandom2014 Beliebers
Êó•Êú¨Âè≤Â´å„ÅÑ„Å†„Çè‚òÜÂΩ°.„ÄÇ
with hulya..
My life is so boring and sadüòí
RT @KUDUNEWS: Karrueche Posts Cryptic Gun Range Pics on IG (@Karrueche @Chrisbrown) - #celebrities #gossip #kudunews http://t.co/U4Ej91H63p
RT @Malefica_1: El lobo siempre ser√° el malo si Caperucita es quien cuenta la historia. #Malefica
RT @miilkkk: üòíüíØ http://t.co/GxXkjHV9KG
Phumolo asks "How much for these apprenticeships? "...Is he  joking? http://t.co/nud7fYA3dP shows SCAM FREE things, you don't pay for jobs!

The default tokeniser for Tweets is `TweetTokenizer`, which is called
by the ``tokenised_tweets()`` method.

>>> for t in reader.tokenised_tweets[:15]:  # doctest: +SKIP
...   print(t)
['Skora', '√∞i', 'aftur', 'en', 'f√©kk', 'ekki', '√≠', 'magann', '.', 'http://t.co/jIvWHkp2wk']
['Ouch', ',', 'likewise', 'üëé']
['@my_future_cat', '—ç—ç—ç—ç', '..', '–Ω—É—É—É—É', '...', '–º–Ω–µ', '17', ',', '–≥—ã']
['@__foreignmarley', 'can', 'you', 'bring', 'the', 'speakers', 'please']
['RT', '@goodhealth', ':', 'Wake', 'up', 'to', 'a', 'healthy', 'breakfast', '!', 'Make', 'your', 'own', 'granola', ',', 'on-the-go', 'bars', ',', 'and', 'parfaits', 'with', 'these', '#recipes', '-->', 'http://t.co/Q6R‚Ä¶']
['#Almer√≠a', '#Trabajo', 'Se', 'necesita', 'repartidores', 'de', 'publicidad', ':', 'Almer', '√≠a', 'Incorporamos', '35', 'personas', 'para', 'distribuir', 'pro', '...', 'http://t.co/uvbPLNRNad']
['@Nenenesssaaa', 'ikaw', 'yon', ',', 'hindi', 'ako', '.', ':p']
['84x', '#MTVHottest', 'Justin', 'Bieber', '#BestFandom2014', 'Beliebers']
['Êó•Êú¨Âè≤Â´å„ÅÑ„Å†„Çè', '‚òÜ', 'ÂΩ°', '.', '„ÄÇ']
['with', 'hulya', '..']
['My', 'life', 'is', 'so', 'boring', 'and', 'sad', 'üòí']
['RT', '@KUDUNEWS', ':', 'Karrueche', 'Posts', 'Cryptic', 'Gun', 'Range', 'Pics', 'on', 'IG', '(', '@Karrueche', '@Chrisbrown', ')', '-', '#celebrities', '#gossip', '#kudunews', 'http://t.co/U4Ej91H63p']
['RT', '@Malefica_1', ':', 'El', 'lobo', 'siempre', 'ser', '√°', 'el', 'malo', 'si', 'Caperucita', 'es', 'quien', 'cuenta', 'la', 'historia', '.', '#Malefica']
['RT', '@miilkkk', ':', 'üòí', 'üíØ', 'http://t.co/GxXkjHV9KG']
['Phumolo', 'asks', '"', 'How', 'much', 'for', 'these', 'apprenticeships', '?', '"', '...', 'Is', 'he', 'joking', '?', 'http://t.co/nud7fYA3dP', 'shows', 'SCAM', 'FREE', 'things', ',', 'you', "don't", 'pay', 'for', 'jobs', '!']

Since this tokeniser assumes that words are delimited by
whitespace, it fails to deal with writing systems such as Japanese
that use different conventions for delimiting words.


(De)Hydration
=============

The authentication steps are the same when using `Query`, a class
which wraps calls to Twitter's REST API. 

.. [McReadie21012] Richard
   McCreadie, Ian Soboroff, Jimmy Lin, Craig Macdonald, Iadh Ounis and
   Dean McCullough (2012) *On Building a Reusable Twitter Corpus*
   `PDF <http://www.dcs.gla.ac.uk/~craigm/publications/mccreadie12tweets.pdf>`__

.. [Morstatter2013] Fred Morstatter, Juergen Pfeffer, Huan Liu and Kathleen
   M. Carley (2013)
   *Is the Sample Good Enough? Comparing Data from Twitter‚Äôs
   Streaming API with Twitter‚Äôs Firehose*
   `PDF <http://www.public.asu.edu/~fmorstat/paperpdfs/icwsm2013.pdf>`__

